{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist.\n",
    "\n",
    "Logistic regression is a model for binary classification predictive modeling.\n",
    "\n",
    "逻辑回归是一种统计模型, 可以用于对二分类数据进行建模, 但也可以扩展到多分类中. 二分类数据指的是需要判断或预估的结果是诸如: 通过/失败, 赢/输, 活着/死了, 健康/生病.\n",
    "\n",
    "In regression analysis, logistic regression is estimation the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled \"0\" and \"1\".\n",
    "\n",
    "In the logistic model, the log-odds (the logarithm of the odds) for the value labeled \"1\" is a linear combination of one or more independent variables (\"predictors\").\n",
    "\n",
    "> 在逻辑模型中, 标记为\"1\"的值的 log-odds (odds 的对数)是一个或多个自变量(\"预测因子\")的线性组合.\n",
    "\n",
    "逻辑回归是拟合出一条线, 将两类分开.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "回归模型可以使用线性代数形式来描述:\n",
    "\n",
    "$$\n",
    "\\vec{y} = X * \\vec{\\beta}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\vec{y}$, 向量, 表示数据集中的输出结果\n",
    "- $X$, 矩阵, 表示数据集中的输入(特征)\n",
    "- $\\vec{\\beta}$, 向量, 表示参数, 也就是我们需要使用模型来寻找的\n",
    "\n",
    "因为回归模型得出的结果是数值而不是分类, 因此需要使用一个非线性函数将其装换成 0 到 1 之间的数.\n",
    "\n",
    "这个非线性函数就是逻辑函数, 也称 sigmoid 函数, 它的公式如下:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "> 上面的公式是逻辑函数的一种特殊形式, 它是设置逻辑函数中的 $L = 1, x_0 = 0, k = 1$\n",
    "\n",
    "将 sigmoid 函数中的自变量 $x$ 替换成权重和(其实就是回归模型):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{1 + e^{-(X * \\vec{\\beta})}}\n",
    "$$\n",
    "\n",
    "我们仔细看看上面的公式中的 $X * \\vec{\\beta}$ 这个实际就是回归模型, 这个公式的结果是数值, 然后通过 sigmoid 函数将数值转换成 0 到 1 之间的值, 这就是概率的区间了\n",
    "\n",
    "The output is interpreted as a probability from a Binomial probability distribution function for the class labeled 1, if the two classes in the problem are labeled 1 and 1.\n",
    "\n",
    "> 如果问题中的两个类分别标记为 0 和 1, 则输出被解释为一个二项式概率分布函数的概率.\n",
    "\n",
    "到现在 $X$ 是数据集中的特征(输入), 我们要做的就是估算模型中 $\\vec\\beta$, 估算方法有两种\n",
    "\n",
    "- Least Squares Optimization\n",
    "- Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and Log-Odds\n",
    "\n",
    "在我们深入模型参数如何靠数据进行估算前, 我们需要了解逻辑回归实际在计算什么.\n",
    "\n",
    "在模型的线性部分, 也就是输入值的权重和计算的是一个成功事件的 log-odds, 需要特别指出的是, 这个 log-odds 是样本属于 1 的分类.\n",
    "\n",
    "概率(probability), 赔率(odds)和对数赔率(log-odds)都是一样的东西, 只是使用了不同的方法来表达.\n",
    "\n",
    "- 概率(probability), 事件发生的概率\n",
    "- 赔率(odds), 事件发生的概率 / 事件不发生的概率, $odds = \\frac{p}{1 - p}$\n",
    "- 对数赔率(log-odds), 赔率的自然对数 $\\ln(odds)$\n",
    "\n",
    "为什么要使用 log-odds? 因为它更容易的由新数据去更新.\n",
    "\n",
    "比如: 假设一个贼有 5% 的几率在晚上进到你的家. 你家的狗有 50% 的几率当贼来的时候会叫, 25% 的几率会不叫. 现在你听到了脚步声, 而且听到了狗叫, 那么这是一个贼的概率有多大?\n",
    "\n",
    "分析步骤:\n",
    "\n",
    "- 在我们听到狗叫前, 是贼的概率为 5%, 转换成 log-odds 为: $\\ln(\\frac{0.05}{0.95}) = -2.9444$, 这个被称为 是贼的先验对数赔率\n",
    "- 狗叫是贼的概率为 1/2, 不是贼的概率 1/4, 转换成 log-odds 为: $\\ln(\\frac{0.5}{0.25} = 0.6931)$, 狗叫是贼的对数赔率\n",
    "- 将两个对数赔率相加为, 是贼的对数赔率, $-2.9444 + 0.6921 = -2.2513$\n",
    "- 所以赔率是 $e^{-2.2513} = 0.1053$\n",
    "- 根据赔率计算概率 $\\frac{0.1053}{1 + 0.1053} = 0.095$\n",
    "- 也就说在听到脚步声且狗叫的情况下是贼的概率是 $9.5%$\n",
    "\n",
    "上面的步骤中用到了概率, 赔率和对数赔率的转换关系\n",
    "\n",
    "- odds = $e^{log-odds}$\n",
    "- p = $\\frac{odds}{1 + odds}$\n",
    "\n",
    "这是根据如下公式推导出来的:\n",
    "\n",
    "- odds = $\\frac{p}{(1 - p)}$\n",
    "- log-odds = $\\ln(odds)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "概率: 0.8\n",
      "赔率: 4.000000000000001\n",
      "根据赔率转换回来的概率: 0.8\n",
      "根据对数赔率转换回来的概率: 0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define our probability of success\n",
    "prob = 0.8\n",
    "# convert probability to odds\n",
    "odds = prob / (1 - prob)\n",
    "# convert odds to log-odds\n",
    "logodds = np.log(odds)\n",
    "# convert back to probability from odds\n",
    "prob_from_odds = odds / (odds + 1)\n",
    "# convert back to probability from log-odds\n",
    "prob_from_logodds = 1 / (1 + np.exp(-logodds))\n",
    "\n",
    "print(f'概率: {prob}\\n赔率: {odds}\\n根据赔率转换回来的概率: {prob_from_odds}\\n根据对数赔率转换回来的概率: {prob_from_logodds}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型的线性部分(输入值的权重和)计算的是成功事件的 log-odds:\n",
    "\n",
    "$$\n",
    "log(odds) = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_mx_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "MLE is a probabilistic framework for estimating the parameters of a model.\n",
    "\n",
    "在 MLE 我们希望在给出观察数据 $X$ 的概率分布和参数 $\\theta$ 后, 能够最大化其条件概率. 可以使用公式表达:\n",
    "\n",
    "$$\n",
    "P(X;\\theta)\n",
    "$$\n",
    "\n",
    "上面的 $X$ 实际上是问题域从 1 到 $n$ 的所有观察数据的联合概率分布.\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, \\cdots, x_n;\\theta)\n",
    "$$\n",
    "\n",
    "条件概率的结果指的是观察数据的似然, 可以表示如下:\n",
    "\n",
    "$$\n",
    "L(X;\\theta)\n",
    "$$\n",
    "\n",
    "联合概率可以认为是条件概率的连乘, 因为概率的连乘会引起数据精度问题, 因此我们使用自然对数来将概率的连乘变成连加.\n",
    "\n",
    "$$\n",
    "\\sum_i^n \\log(P(x_i;\\theta))\n",
    "$$\n",
    "\n",
    "上面的式子通常指的是 log-likelihood 函数, 我们使用取反操作使得取最小值\n",
    "\n",
    "$$\n",
    "\\text{minimize} \\sim -\\sum_i^n \\log(P(x_i;\\theta))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression as Maximum Likelihood\n",
    "\n",
    "在机器学习中, 通常我们使用 $h$ 来代替 $\\theta$, $h$ 表示 *hypothesis*. 也就是说寻找模型的 *hypothesis* 就是最大化 likelihood function.\n",
    "\n",
    "$$\n",
    "\\text{maximize} \\sim \\sum_i^n \\log(P(x_i;h))\n",
    "$$\n",
    "\n",
    "监督学习可以被框定为一个条件概率问题, 即在给定输入的情况下预测输出的规律.\n",
    "\n",
    "$$\n",
    "P(y|X)\n",
    "$$\n",
    "\n",
    "如此, 我们能够为监督学习定义条件最大化似然估计.\n",
    "\n",
    "$$\n",
    "\\text{maximize} \\sim \\sum_i^n \\log(P(y_i|x_i;h))\n",
    "$$\n",
    "\n",
    "现在我们可以替换 $h$ 为我们的逻辑回归模型.\n",
    "\n",
    "为了使用最大似然, 我们需要假设概率分布. 在逻辑回归的情况下, 数据样本(data sample)假设是一个二项式概率分布的, 每一个样例(example)都是一次伯努利实验的结果. 伯努利分布只有一个参数: 成功结果的概率 ($p$)\n",
    "\n",
    "- $P(y=1) = p$\n",
    "- $P(y=0) = 1 - p$\n",
    "\n",
    "伯努利分布的期望值可以使用如下公式计算:\n",
    "\n",
    "$$\n",
    "\\text{mean} = P(y=1) * 1 + P(y=0) ) * 0\n",
    "$$\n",
    "\n",
    "或者在给定 $p$ 的情况下, 上面的公式可以写为:\n",
    "\n",
    "$$\n",
    "\\text{mean} = p * 1 + (1 - p) * 0\n",
    "$$\n",
    "\n",
    "这种计算可能看起来是多余的, 但它为特定输入的似然函数提供了基础, 其中的概率由模型$\\hat{y}$给出, 时间段标签有数据集给出\n",
    "\n",
    "$$\n",
    "\\text{likelihood} = \\hat{y} * y + (1-\\hat{y}) * (1-y)\n",
    "$$\n",
    "\n",
    "这个函数将总是返回更大的概率, 当模型接近匹配的分类值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=1, yhat=0.9, likehood: 0.900\n",
      "y=1, yhat=0.1, likehood: 0.100\n",
      "y=0, yhat=0.9, likehood: 0.100\n",
      "y=0, yhat=0.1, likehood: 0.900\n"
     ]
    }
   ],
   "source": [
    "# test of Bernoulli likelihood function\n",
    "\n",
    "# likelihood function for Bernoulli distribution\n",
    "def likelihood(y, yhat):\n",
    "  return yhat * y + (1 - yhat) * (1 - y)\n",
    "\n",
    "# test for y=1\n",
    "y, yhat = 1, 0.9\n",
    "print(f'y={y}, yhat={yhat}, likehood: {likelihood(y, yhat):.3f}')\n",
    "y, yhat = 1, 0.1\n",
    "print(f'y={y}, yhat={yhat}, likehood: {likelihood(y, yhat):.3f}')\n",
    "# test for y=0\n",
    "y, yhat = 0, 0.9\n",
    "print(f'y={y}, yhat={yhat}, likehood: {likelihood(y, yhat):.3f}')\n",
    "y, yhat = 0, 0.1\n",
    "print(f'y={y}, yhat={yhat}, likehood: {likelihood(y, yhat):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将 likelihood function 转换成 log-likelihood function\n",
    "\n",
    "$$\n",
    "\\text{logLikelihood} = \\log(\\hat{y}) * y + \\log(1 - \\hat{y}) * (1 - y)\n",
    "$$\n",
    "\n",
    "最后我们可以根据所有在数据集中的样例(examples)合计他们的 likelihood function, 实现最大似然:\n",
    "\n",
    "$$\n",
    "\\text{maximize} \\sim \\sum_i^n \\log(\\hat{y_i}) * y_i + \\log(1-\\hat{y_i}) * (1-y_i)\n",
    "$$\n",
    "\n",
    "通常在实践中为了优化问题我们需要最小化 cost function; 因此我们可以转换函数去实现最小化, 即 NNL(negative log-likelihood).\n",
    "\n",
    "$$\n",
    "\\text{minimize} \\sim -\\sum_i^n \\log(\\hat{y_i}) * y_i + \\log(1-\\hat{y_i}) * (1-y_i)\n",
    "$$\n",
    "\n",
    "计算伯努利分布的 NNL 等于是计算伯努利分布的交叉熵函数(cross-entropy function), $p()$ 表示分类 0 或分类 1 的概率, $q()$ 表示概率分布的估算.\n",
    "\n",
    "$$\n",
    "\\text{crossEntropy} = -(\\log(q(class_0)) * p(class_0) + log(q(class_1)) * p(class_1))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [WikiPedia](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "- [A Gentle Introduction to Logistic Regression With Maximum Likelihood Estimation](https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/#:~:text=The%20maximum%20likelihood%20approach%20to,fitting%20classification%20models%20more%20generally.)\n",
    "- [Log Odds: Simple Definition & Examples, Conversions](<https://www.statisticshowto.com/log-odds/#:~:text=Log%20odds%20is%20the%20logarithm,4)%20%3D%201.38629436%20%E2%89%85%201.386.>)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c47b3fd94286b9788ca24a5dc76812a7f3d81a041a874dbfccf1db12dc8e64b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
